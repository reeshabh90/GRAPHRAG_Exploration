{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample notebook about generating graph from a plain LLM model without GRAPHRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"neo4j://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"neo4jpassword\"\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAI, AzureChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"Deployment name\",\n",
    "    openai_api_type=\"azure\",\n",
    "    api_key=\"API-KEY\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=\"BASE URL\",\n",
    "    temperature=0, # temperature has to be kept at 0 as we are not doing any creative writing and hence want out llm to be grounded.\n",
    ")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDF Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n \\n \\nReeshabh Choudhary \\n \\n \\nCONCURRENCY & \\nPARALLELISM \\nEfficient Resource Utilization in Programming \\n \\n \\n \\n      \\nREESHABH CHOUDHARY \\n      \\nConcurrency & Parallelism \\n \\n2 \\n \\nConcurrency & Parallelism © 2024 by Reeshabh Choudhary , all rights reserved. \\n \\n \\n \\nReeshabh Choudhary \\n \\n \\n \\nया क\\nुन्देन्दुतुषारहारधवला या शुभ्रवस्त्रावृता \\nया वीणावरदण्डमण्ण्डतकरा या श्वेतपद्मासना। \\nया ब्रह्माच्युत शंकरप्रभृततभभदेव ैः सदा वण्न्दता \\nसा मां पातु सरस्त्वती भगवती तनैःशेषजाड्यापहा॥ \\n(Salutations to Devi Saraswati) Who is pure white like jasmine, with the \\ncoolness of the moon, \\nthat shines like a garland of snow and pearls; And who is covered with \\npure white robes, \\nWhose hands are adorned with veenas and boons; And who sits on a \\npure white lotus, \\nOne who is always the adoration of Bhagwan Brahma, Bhagwan Vishnu, \\nBhagwan Shankar and other deities, \\nO Devi Saraswati, please protect me and remove my ignorance \\ncompletely. \\n \\n \\nConcurrency & Parallelism \\n \\n4 \\n \\n \\n \\nReeshabh Choudhary \\n \\nAcknowledgements \\nI bow down to my deities Maa Saraswati, Ganesh ji and Hanuman ji for guiding \\nme to pursue knowledge and giving me strength to deal with challenges on the \\nway. \\nThe journey from my first book Objects, Data & AI to the one you are reading \\ncurrently has been nothing sort of ordinary. Having completed my first book \\naround July 2023, I was not ready to take up the task of compiling another \\nbook for a considerable period of time. In the meantime, me and Richa have \\nbeen blessed with a baby girl on 5th September 2023. And it was her journey \\nwhich inspired me to take up writing again. \\nOn the second day after her birth, she was detected with NEC (Necrotizing \\nenterocolitis), which affected her large intestine. NEC is usually detected in pre-\\nmatured babies, however, our baby being full term somehow got infected by it. \\nShe had to be operated a week later, and required another procedure, which \\nwas done successfully in December 2023. By the grace of Supreme Being, and \\nher spirit to survive, she is now completely normal and lights up every moment \\nof our life. The journey of this 4-month duration had been nothing sort of a \\nrollercoaster ride though. Sometimes our own words are enough to describe \\nwhat we go through, but the humankind has been blessed with writers who \\ncan capture the essence of emotion through magic of words.  Which lines to \\nselect out of many written, perhaps these from Charles Dickens resonate more: \\n“Oh! the suspense, the fearful, acute suspense, of standing idly by while \\nthe life of one we dearly love, is trembling in the balance! Oh! the racking \\nthoughts that crowd upon the mind, and make the heart beat violently, and \\nthe breath come thick, by the force of the images they conjure up before it; \\nthe desperate anxiety to be doing something to relieve the pain, or lessen the \\ndanger, which we have no power to alleviate; the sinking of soul and spirit, \\nwhich the sad remembrance of our helplessness produces; what tortures \\ncan equal these; what reflections or endeavours can, in the full tide and \\nfever of the time, allay them! ” \\n- Oliver Twist, Chapter XXXIII \\nAnd it was in this period, to keep myself composed and clear, free from \\ndistractions and thoughts of uncertain future over which I hold no control \\nwhatsoever, I decided to research into the topic of Concurrency and Parallelism.  \\nConcurrency & Parallelism \\n \\n6 \\n \\nAs they say there is light at the end of the tunnel, darker days indeed passed and \\nsun smiled over us, and in January 2024, I started compiling my learnings in a \\nstructured format which is being presented before you.  \\nThe immortal words of Fyodor Dostoevsky summarize it better: \\n“The soul’s journey is often fraught with trials and tribulations. Yet, it is \\nprecisely through our struggles that we uncover the depths of our \\ncharacter and the breadth of our capacity for love and compassion.” \\nI cannot thank enough my dear friends Mr. Ashish Agarwal, Mr. Gurpreet Singh, \\nand Mr. Saurabh Singh, who firmly stood by us throughout the journey. In the \\nprocess of writing the book, Mr. Saurabh Singh kept pushing me to go deeper \\nand shared some invaluable learning resources which were helpful in adding \\ncontext to certain topics. \\nAs always, I had the blessing of my mentor, Mr. Rupam Das, who consciously \\nencouraged me to research into a list of topics, which would later help to enrich \\nthe content of the material which you are about to read. \\nI drew a lot of inspiration from the writings of Mr. Robert Greene, as it helped to \\nme have faith on my abilities, chose the path less travelled by and trust my \\ninstincts. His writings acted as my inner voice which I intended to listen time and \\nagain. \\nThis compilation would have never been possible without the strength of my \\nbeloved wife, Richa, who trusted on my decisions blindly and made sure I had \\nenough space to work on this project. \\nAnd Maithili, you have fought against the odds and thrived. May you continue to \\ndo so, as you are the legacy which I intend to leave behind. Your smile heals me \\nevery single day. I love you both! \\n \\n \\n \\nReeshabh Choudhary \\n \\nFor Maithili! \\n \\n \\n \\nConcurrency & Parallelism \\n \\n8 \\n \\nPreface \\nYear 2023 saw the rise of Large Language models (GPT, BARD, etc.) and smart \\ncoding assistants (GitHub Copilot). Since then, a fear has been gripping in the \\nmind of software developers that this is the end of programming as we know it. \\nAfter all, these models can generate hundreds of lines of code on the context \\nof few lines of prompt. But programming is more than just writing lines of \\ncode. This is an art, and it requires you to think through the system, and this is \\nthe missing link in the artificial intelligence. It can always be trained on past \\ndata and based on that it can predict or generate content, however, it cannot \\nbe held accountable or be taken granted for writing the most efficient program \\nwhich suits your need.  \\nSay, I have a task which requires me to compare a document against a set of \\ndocuments on various parameters. I ask an AI model to write a program for this \\ntask. However, this is most likely going to be a sequential program. Now, if you \\nwant to utilize available cores of your CPU and complete the task in a shorter \\nspan of time, you might think of parallelizing the computations in your \\nprogram. This requires identifying the pieces of your program which can be \\nparallelized and do not result in synchronization overhead. Once, you are able \\nto do that, you may ask the AI model to modify your program in a certain way. \\nThe output generated can be then integrated as per your requirements. \\nThe point which I am trying to make here is that to write a good program, a \\ndeveloper does not just need to have an understanding of programming \\nlanguages but must understand the capabilities of the system where program is \\ngoing to run. The AI model can no doubt quickly generate lines of code, but it \\nstill requires direction of a human mind. \\nMachines can be trained to perform certain tasks, but this is not the guarantee \\nthat the tasks are being performed in the most efficient manner. It is called \\nartificial intelligence for a reason. To think through the systems, adapt and \\nimprovise, to be held accountable, human intelligence is required and that \\nshould be the case in the future unless we are ruled by humanoids.  \\n \\n \\nReeshabh Choudhary \\n \\nWhat this book is about? \\nThis book is about developing an intuition about what happens underneath a \\nprogram, so that we can first focus on utilizing the available resource at hand \\nbefore thinking of scaling the solution. This is the very art of engineering, \\nmaking use of resources in limited budget and getting things done. Once a \\ndeveloper understands what is going on beneath the surface, the programming \\nlanguages are nothing but just abstractions via which we talk to our computers. \\nAnd this is one of the reasons why this book has used minimal programming \\nlanguage. Rather than focussing on programming language, once a software \\ndeveloper starts to think with respect to the device at hand, then the real \\njourney of programming starts.  \\nThis book just barely tries to scratch the surface of the vast world of \\nprogramming and starts from the very basic concepts of Operating Systems and \\nthen moving on to application development and in the second part interaction \\nwith the databases is covered.  \\nRight Approach to read this book. \\nI will request the readers of this material to have a research centric approach \\nwhile reading it. At times, there are lots of concepts which interleave and for \\nthe sake of brevity, author has skipped explaining some of them to maintain \\nthe flow of the discussion. User is encouraged to leverage online search to look \\nfor concepts which require elaborate explanation.  \\nEvery reader has the ability to connect the dots in a unique way based upon \\nthe understanding and experience developed over the years. And for the same \\nreason, the discussion presented in this book may seem quite open ended. \\nThere is no fixed approach to programming however underlying resources like \\nthreads or processes remain the same. This is why different programming \\nlanguages or different frameworks within a programming language coexist.  \\nI have humbly tried to make this point to my precious readers that every \\ndecision, whether in real life or technical design, comes with its pros and cons. \\nThere is no fixed answer, and we must stop looking for them. What works for \\none may not work for other use case. We must practice the ability of \\nscrutinizing our choice and be fluid in selecting appropriate approaches to \\nprogramming, rather than being biased towards an approach or language or \\nframework. As Robert Greene aptly puts it: \\nConcurrency & Parallelism \\n \\n10 \\n \\n“The need for certainty is the greatest disease the mind faces.” – \\nMastery, Robert Greene. \\nThe book tries to present different case studies of frameworks and application \\ndevelopment to present the readers with different ideas. Rather than \\npreaching a way of programming, book tries to weigh pros and cons of \\ndifferent concepts and also explores how the concepts intermingles with each \\nother. The onus of implementing these concepts in day-to-day programming \\nlies with the reader.  \\nA book is always about presenting a story to its reader, whether fictional or \\nnon-fictional. The material you are reading right now is focussed on presenting \\nthe story of evolution of programming and the way forward! \\nNOTE: I am merely a compiler of the knowledge and information \\nalready available in the public domain. I do not intend to take any \\ncredit for the work done by amazing programmers over the years. This \\nbook is all about appreciating the programmers who think of utilizing \\nthe system resources at maximum and deliver an efficient yet robust \\nproduct. \\n \\nReeshabh Choudhary \\n \\nReferences \\nThroughout human history, books have been the biggest source of knowledge. \\nA good book inspires countless other books. A student reads different books on \\nsame topic to understand different perspectives. I am no different. Compilation \\nof this book would have been incomplete without numerous books and online \\nsource materials present. Although, it would not be possible to mention each \\nand every source, from where I have gathered information, I would definitely like \\nto mention some key books which have served as the axis of this book. \\n\\uf0d8 The Art of Multiprocessor Programming - Maurice Herlihy, Nir Shavit \\n\\uf0d8 Concurrency Control and Recovery in Database Systems - Philip A. \\nBernstein, Vassos Hadzilacos, Nathan Goodman  \\n\\uf0d8 The Little Book of Semaphores, The Ins and Outs of Concurrency Control \\n- Allen B. Downey \\n\\uf0d8 Fundamentals of Database Systems - Ramez Elmasri, Shamkant B. \\nNavathe \\n\\uf0d8 Database system concepts - Henry F. Korth, Abraham Silberschatz, S. \\nSudarshan  \\n\\uf0d8 Principles of distributed database systems - Ozsu M.T., Valduriez P \\n\\uf0d8 Programming Massively Parallel Processors, A Hands-on Approach - \\nDavid B. Kirk, Wen-mei W. Hwu \\n\\uf0d8 The Art of Concurrency, A Thread Monkey's Guide to Writing Parallel \\nApplications - Clay Breshears \\n\\uf0d8 ACTORS, A Model of Concurrent Computation in Distributed Systems - \\nGul Agha \\n\\uf0d8 Actors in Scala - Philipp Haller, Frank Sommers \\n\\uf0d8 Is Parallel Programming Hard, And, If So, What Can You Do About It? - \\nPaul E. McKenney \\n\\uf0d8 Operating System Concepts - Abraham Silberschatz, Greg Gagne, Peter B. \\nGalvin \\nApart from these books, I gathered lot of reading material via help of \\nstackoverflow.com and perplexity.ai . I would humbly like to thank the software \\nengineers who have kept sharing their wisdom over the years via blogs to help \\nfellow engineers across the globe. In my personal opinion, apart from sports, \\nprogramming community is definitely the one which brings people together \\nacross boundaries.  \\n \\nConcurrency & Parallelism \\n \\n12 \\n \\n \\n \\nReeshabh Choudhary \\n \\nContents \\n Introduction .................................................................................................... 19 \\n1.1 Concurrency Vs Parallelism \\n..................................................................... 21 \\n Beneath the surface: OS \\n.................................................................................. 26 \\n2.1 Evolution of Computer Architecture \\n....................................................... 29 \\n2.2 Interaction with a CPU Core ................................................................... 34 \\n2.3 Process ................................................................................................... 38 \\n2.4 Inter-process Communication (IPC) ........................................................ 44 \\n2.5 Threads .................................................................................................. 47 \\n2.6 Threading Models .................................................................................. 48 \\n2.7 Thread Scheduling \\n.................................................................................. 50 \\n2.7.1 Single CPU Core system ................................................................... 50 \\n2.7.2 Multiple Processor system \\n............................................................... 51 \\n2.7.3 Multi-Core system ........................................................................... 52 \\n2.8 Use of Thread Pools ............................................................................... 54 \\n2.9 Multi-threading and Parallel programming ............................................ 56 \\n2.10 Adoption of GPUs \\n................................................................................. 61 \\n2.10.1 GPU Architecture \\n........................................................................... 65 \\n2.11 Summary \\n.............................................................................................. 68 \\n Order to Chaos: Synchronization ..................................................................... 70 \\n3.1 Race Condition ....................................................................................... 70 \\n3.2 Synchronization Strategies ..................................................................... 73 \\n3.2.1 Hardware level Solutions ................................................................. 73 \\n3.2.2 Atomic Operations \\n........................................................................... 81 \\n3.2.3 Software level solutions................................................................... 86 \\n3.3 Cost and Considerations about Synchronization \\n................................... 105 \\n3.4 Practical Scenario ................................................................................. 105 \\n3.5 Summary \\n.............................................................................................. 106 \\n Beyond Thread Synchronization .................................................................... 108 \\nConcurrency & Parallelism \\n \\n14 \\n \\n4.1 Asynchronous approach ....................................................................... 108 \\n4.1.1 Use of Event Loop \\n.......................................................................... 112 \\n4.1.2 Asynchronous nature of Node JS ................................................... 114 \\n4.1.3 Curious Case of Redis .................................................................... 120 \\n4.1.4 Limitations of Asynchronous approach \\n.......................................... 127 \\n4.2 Programming for Immutable State ....................................................... 128 \\n4.3 Functional Programming ...................................................................... 129 \\n4.3.1 Higher Order Functions ................................................................. 131 \\n4.3.2 Lazy Evaluation .............................................................................. 132 \\n4.3.3 Limitations \\n..................................................................................... 142 \\n4.4 Actor Model ......................................................................................... 145 \\n4.4.1 Notion of State in Actor Model \\n...................................................... 154 \\n4.4.2 Threads and Actors \\n........................................................................ 155 \\n4.4.3 Fault Tolerance .............................................................................. 155 \\n4.4.4 Actor Model and Distributed Systems ........................................... 156 \\n4.5 Summary \\n.............................................................................................. 157 \\n Towards Parallelism \\n....................................................................................... 160 \\n5.1 Designing a Parallel Program ................................................................ 161 \\n5.1.1 Task Decomposition ...................................................................... 162 \\n5.1.2 Agglomeration \\n............................................................................... 166 \\n5.1.3 Task Mapping ................................................................................ 167 \\n5.1.4 Data Decomposition ...................................................................... 168 \\n5.2 Example: Edge Detection in Image Processing \\n...................................... 169 \\n5.3 Performance Evaluation of Parallel Programming................................. 173 \\n5.3.1 Performance Factors ..................................................................... 173 \\n5.3.2 Performance Metrics ..................................................................... 179 \\n5.3.3 Amdahl’s Law ................................................................................ 184 \\n5.3.4 Gustafson’s law.............................................................................. 185 \\n5.3.5 Energy Usage \\n................................................................................. 185 \\n5.4 Recent Trend in Parallel Programming \\n.................................................. 188 \\nReeshabh Choudhary \\n \\n5.4.1 Case Study: Parallelization of Neural Networks ............................. 189 \\n5.5 Summary \\n.............................................................................................. 194 \\n Centralized Database Systems \\n....................................................................... 198 \\n6.1 DBMS Architecture \\n............................................................................... 199 \\n6.2 Transactions ......................................................................................... 201 \\n6.3 Concurrency Problems ......................................................................... 205 \\n6.3.1 Dirty Read...................................................................................... 205 \\n6.3.2 Non-repeatable Read .................................................................... 205 \\n6.3.3 Phantom Read ............................................................................... 206 \\n6.3.4 Lost Update ................................................................................... 206 \\n6.3.5 Dirty Write \\n..................................................................................... 208 \\n6.3.6 Write Skew .................................................................................... 209 \\n6.4 Schedule and Serializability .................................................................. 209 \\n6.5 Isolation Levels \\n..................................................................................... 214 \\n6.5.1 Read Uncommitted........................................................................ 215 \\n6.5.2 Read Committed \\n............................................................................ 215 \\n6.5.3 Repeatable Read/Snapshot Isolation ............................................. 217 \\n6.5.4 Parallel Snapshot Isolation............................................................. 220 \\n6.5.5 Serializable .................................................................................... 222 \\n6.5.6 Serializable Snapshot Isolation ...................................................... 222 \\n6.6 Locking Mechanism \\n.............................................................................. 224 \\n6.6.1 Two-Phase Locking Protocol .......................................................... 227 \\n6.6.2 Deadlock Prevention and Resolution ............................................. 228 \\n6.7 Multi Version Concurrency Control ...................................................... 231 \\n6.7.1 Timestamp Ordering Protocol ....................................................... 232 \\n6.7.2 Implementing MVCC using timestamp ordering ............................ 235 \\n6.8 Summary \\n.............................................................................................. 236 \\n Parallel Processing in Database Systems ....................................................... 238 \\n7.1 Parallelism in Databases ....................................................................... 240 \\n7.2 Inter-Query Parallelism \\n......................................................................... 241 \\nConcurrency & Parallelism \\n \\n16 \\n \\n7.3 Intra-Query Parallelism \\n......................................................................... 241 \\n7.3.1 Intraoperation Parallelism ............................................................. 242 \\n7.3.2 Interoperation Parallelism ............................................................. 251 \\n7.4 The Right Approach \\n.............................................................................. 252 \\n7.4.1 Cost of Optimization \\n...................................................................... 253 \\n7.4.2 Colocation of Data ......................................................................... 255 \\n7.5 Summary \\n.............................................................................................. 258 \\n Transactions in Distributed Database Systems .............................................. 260 \\n8.1 Distributed Transaction Processing \\n....................................................... 260 \\n8.1.1 2-Phase Commit Protocol .............................................................. 262 \\n8.1.2 3-Phase Commit Protocol .............................................................. 264 \\n8.1.3 Practical Approach \\n......................................................................... 265 \\n8.1.4 Consensus Algorithms ................................................................... 266 \\n8.2 Locking Mechanism & Deadlock \\n........................................................... 274 \\n8.3 Timestamp generation and ordering .................................................... 277 \\n8.4 Distributed Snapshot Isolation ............................................................. 279 \\n8.5 Summary \\n.............................................................................................. 280 \\n \\n \\nReeshabh Choudhary \\n \\n \\n \\nConcurrency & Parallelism \\n \\n18 \\n \\n \\nPart 1: OS & Application Development \\n \\n \\nReeshabh Choudhary \\n \\n Introduction \\n \\nThe word concurrency is derived from the Medieval Latin word “concurrentia”, \\nmeaning coming together or simultaneous occurrence, which itself has evolved \\nfrom the Latin word “concurrens”, meaning running together. The word \\nconcurrency has been used since the last 15th century, and we can assume that \\nthe concept of coming together at the same time would have existed much \\nbefore that. \\nThe word parallelism comes from the word ‘parallel’, which is derived from the \\nGreek word ‘parallēlos’ (para = alongside, allēlos = one another). It has been \\nrecorded in use since 16th century.  \\nThe point being, we are not new to these terms or their usage.  \\nImagine a huge crowd at the ticket counter of a newly released \\nmovie or customers lining up in the bank branches at the end of \\nthe month to withdraw salaries. If a resource is valuable, scarce \\nand in demand, it is meant to face concurrent scenarios. And to \\nserve these demands, a theatre or bank may arrange for \\noperation of parallel counters to increase the efficiency and \\nspeed of service. \\nWhat we do in the computer world (virtual world), is merely replicate the entities \\nand their functioning in the real world. The idea and inspiration behind the \\nsolutions implemented in software systems are always derived from the \\nobservations and situations happening in real life.  \\nImagine a chef working in a famous restaurant and dealing with the surge of \\ncustomers on a weekend. He would adopt different strategies to deal with \\nsituations. A recipe has multiple sub-tasks to be performed, which can be either \\na result of one another or some steps can be performed parallelly. A chef can \\ndelegate parallel steps to other workers in the kitchen and once the task is \\ncompleted, he would combine the outputs and produce the desired cuisine. Or \\nhe can create parallel workstations, where orders are prepared end to end, and \\nhe can oversee their preparations. There can be other possible approaches to \\ndeal with the situation. \\nConcurrency & Parallelism \\n \\n20 \\n \\nIn real life or virtual, there is no one stop solution. Every solution comes with its \\nown after-effects, and it is up to us to adapt what seems the best suited for our \\nsituation. The solutions we are using today have evolved over time, after learning \\ndifferent outcomes in different situations. \\nDealing with concurrency in the computer world is no different. Three decades \\nback, the resources inside a computer were not compute heavy, had limited \\ncompute capability and processing power. In fact, early computers had just one \\nprocessor, but evolved over time to perform multiple tasks simultaneously. The \\nsector of computer chip design also went through a significant transformation.  \\nIn 1965, Gordon Moore, the co-founder of Intel, published a paper in which he \\nobserved that the number of transistors on an integrated circuit at minimum cost \\nhad increased by a factor of two between 1960 and 1965. Based on this \\nobservation, he further predicted that the number of transistors on an \\nintegrated circuit will double every two years with minimal rise in cost.  \\nHis observation was not scientific but based on intuitive understanding of this \\nfield, and over time his observation has stood the test of time. And the \\nmicroprocessors of Intel Pentium family further cemented his legacy by standing \\ntrue to his observations. These microprocessors based on a single central \\nprocessing unit (CPU), drove rapid performance increases and cost reductions in \\ncomputer applications for more than two decades, which allowed application \\nsoftware to provide more functionality, have better user interfaces, and generate \\nmore useful results. However, customer demands are rarely satisfied, and \\nexpectation increases at each turn of the wheel. The ever-increasing demand for \\nperformance improvement and cutthroat competition in this field, led to a \\npositive cycle for computer industry. \\nSoftware developers started relying on the underlying hardware to increase the \\nspeed of the application. However, since 2003, due to energy consumption and \\nheat-dissipation issues, processor’s clock speed started hitting the upper limit. \\nVendors started switching towards models where multiple processing units, \\nprocessor cores, are used in each chip to increase the processing power. And this \\nchanged the course of programming constructs since then.  \\nTraditionally, software developers approached programming in sequential \\nfashion, as if programs are executed by a human sequentially stepping through \\nthe code. However, a sequential program will only run on one of the processor \\ncores, which has practically hit the ceiling in current times. But the demand for \\nReeshabh Choudhary \\n \\nimprovement in performance in software application has been constant. Hence, \\nin modern approach to programming, software developers have started to \\nexplore the concept of parallel programming, which basically means to increase \\nthe processing power of a computer by leveraging CPU cores in parallel.  \\nThe emphasis now is on parallelism – rather than using transistors to increase \\nclock speeds, they are being allocated more cores and low-latency memory onto \\nthe chip. With parallelism, large computational tasks are divided into smaller \\ntasks that can be executed simultaneously on different cores. This allows for \\nbetter utilization of available resources and improved overall performance, \\nalthough with some downsides, which are going to be the topic of discussion \\nthroughout the book.  \\nOften the part where computers perform multiple tasks simultaneously creates \\na blur line of concurrency and parallelism. Hence, we try to address this \\nconfusion before starting any serious in-depth discussion. \\n1.1 Concurrency Vs Parallelism \\nTo understand this subtle difference between concurrent and parallel operations \\nin computer, we must understand their evolution over time. Early age computers \\nhad just one processor, hence, to perform multiple operations at the same time, \\nit had to adopt some smart strategies like switching between tasks as they come \\n(concurrent) and then resuming the operations from where it has left earlier. \\nHowever, with time computers started getting multiple processing power \\n(multiple CPUs) and they had access to more resources to handle tasks at hand. \\nHence, the computers could parallelly perform sub tasks of a major task through \\navailable CPUs or two independent tasks could be performed parallelly.  \\nTo be precise, Concurrency refers to the ability of different tasks to progress in \\noverlapping time periods. In a concurrent system, multiple tasks are initiated, \\nexecuted, and completed, but they may not necessarily be executed \\nsimultaneously. Concurrency is often associated with systems where tasks are \\ninterleaved, sharing the same resources or time slices. Multitasking on a single-\\ncore processor is an example of concurrency, where the CPU rapidly switches \\nbetween different tasks, giving the illusion of parallel execution. \\nOn the other hand, Parallel execution implies that tasks are literally running at \\nthe same time. In a parallel system, multiple processors, cores, or execution units \\nwork simultaneously to execute different parts of a task. This can lead to a \\nsignificant increase in processing speed and throughput. Examples of parallelism \\nConcurrency & Parallelism \\n \\n22 \\n \\ninclude executing multiple threads simultaneously on a multicore processor or \\ndistributing computation across multiple machines in a cluster. \\n \\nFigure 1-1 Concurrency versus parallelism \\nLet us go back to our kitchen example. Imagine a restaurant kitchen where \\ndifferent team members are working on various tasks concurrently. One person \\nis toasting bread, another is frying eggs, and yet another is preparing coffee. \\nEach team member is focused on their specific task, and their activities are \\ninterleaved. While one person is waiting for the toast, another is actively working \\non the eggs or coffee. The kitchen team efficiently manages their tasks, and the \\noverall breakfast preparation progresses in overlapping time periods. \\nWith more demand coming to the kitchen, they decided to upgrade the kitchen \\nworkstation. The upgraded kitchen setup has multiple cooking stations, each \\nequipped with its own stove, toaster, and coffee maker. In this setup, different \\nteam members can work simultaneously on their specific tasks without waiting \\nfor shared resources. One person is toasting bread at Station 1, another is frying \\neggs at Station 2, and a third team member is brewing coffee at Station 3. The \\ntasks are running in parallel, and the overall breakfast preparation is completed \\nmore quickly because multiple activities are happening at the same time. \\nOf course, what we are discussing here is vague and we shall be covering the \\ndetailed explanation in the upcoming chapters about how computers leverage \\nresources to produce desired results, but we get an idea of concurrent and \\nReeshabh Choudhary \\n \\nparallel usage of a resource. Often, we approach a solution with resource cost in \\nmind.  \\nParallelism often involves multiple resources, such as additional processors, \\ncores, or servers. These resources come with associated costs, both in terms of \\nhardware and potentially increased operational expenses. Understanding the \\nresource requirements and costs associated with parallelism is essential to make \\ninformed decisions about the project budget and resource allocation. \\nWe try to maximize the utilization of available resources and still if we require \\nmore helping hands, we deploy more resources (scaling). Our discussion \\nthroughout the book will be mainly focused on efficient utilization of available \\nresources and their coordination in completing an activity over time during \\nconcurrent situations. Scalability is a beast on its own and requires a separate \\ndiscussion. In this book we mostly try to look at challenges from the perspective \\nof a single computer system. \\nWhile parallelism can enhance scalability, it needs to be planned thoughtfully. \\nBlindly adding resources without understanding the scalability characteristics of \\nthe application can lead to suboptimal performance and wasted resources. \\nWorkload demands can vary over time, and concurrency management plays a \\nkey role in adapting to these variations.  \\nParallelism is about leveraging the available computational power of the \\nprocessing cores, if sitting idle. This is where these two concepts interleave. \\nConcurrency is about managing and utilizing resources efficiently. Understanding \\nhow concurrency works at various levels, from operating systems to application \\nframeworks, helps in designing systems that make the most effective use of \\navailable hardware. This includes optimizing algorithms, minimizing contention \\nfor shared resources, and reducing unnecessary overhead. It allows developers \\nto design systems that can dynamically scale resources based on demand, \\npreventing unnecessary resource allocation during periods of lower demand, \\nand ensuring responsiveness during peak periods. Concurrency presents a more \\nstrategic approach to scalability planning and ensures that resources are added \\nin a way that aligns with the actual needs of the application. \\nSay, we break down a large computational task into independent subtasks which \\nare then being parallelly processed at different cores. But, to process them \\nefficiently, we may end up utilizing the knowledge of concurrency in OS, as the \\nsubtask can itself require a process to create multiple threads for processing the \\nConcurrency & Parallelism \\n \\n24 \\n \\ninstructions. We discuss about ‘process’ and ‘threads’ in upcoming chapter, but \\nwe can draw the kitchen analogy just discussed here. \\nAlthough modern programming languages, libraries, and frameworks provide \\nhigh-level abstractions for concurrent and parallel programming, a deeper \\nunderstanding of the underlying concepts is crucial for building compute-\\nintensive applications with efficient resource management. The abstraction \\nprovided by the programming languages does not expose all the nuances of \\nperformance optimization. To achieve optimal performance and resource \\nutilization, it is crucial to understand how concurrent and parallel constructs are \\nimplemented at the lower levels of the system. This includes knowledge of how \\nthreads are scheduled, how processes share memory, and how system resources \\nlike CPU, memory, and I/O are managed. This understanding helps developers \\nwrite code that makes efficient use of available resources.  \\nIt is important to note here that the term ‘concurrency’ in \\ncomputer programing is not just about how the processes or \\nthreads work internally, but also about the concurrent requests \\nan application or database serves at one point of time. \\nThroughout the discussion, this context does get overlapped, so \\nplease keep an eye out. \\nThe aim of the discussions presented in this book is to set the ground for \\nmaximum resource utilization in a shared memory computer and understand the \\nconflicting scenarios which occur in the process. In the upcoming chapter, we \\nshall be starting our discussion from the basic computer architecture and \\ncomponents and how they interleave to perform tasks, and in subsequent \\nchapters we build upon these concepts.  \\n \\n \\nReeshabh Choudhary \\n \\n \\n \\n \\nConcurrency & Parallelism \\n \\n26 \\n \\n Beneath the surface: OS \\n \\nWhen a user interacts with a computer, he/she does seem to interact with an \\nentity, which itself is an abstraction over multiple components functioning \\ntogether.  Instructions are provided via external devices (hardware) such as \\nkeyboard, mouse, etc., in the form of application programs (text editors, \\nspreadsheets, calculator, etc.), which are meant to perform the tasks intended \\nby the user. \\nFrom computer’s perspective, it is the Operating System which abstracts away \\nthe functioning of computer hardware underneath and presents to user an \\ninterface to collect instruction and get them executed using the resources of \\ncomputer.  \\n \\nFigure 2-1 User interaction with computer \\nThe hardware of a computer encapsulates the CPU (central processing unit), the \\nmemory and the input/output devices, and provides resources for \\ncomputational ability. Operating System (OS) provides the necessary \\nenvironment for controlling and allocating these resources to perform \\ncomputational tasks provided by the user. The main aim of computer systems is \\nto execute programs and to help users solve their problems easily. Operating \\nSystem can be viewed as a program (kernel) that runs on the computer all the \\ntime, running alongside a middleware framework that ease application \\ndevelopment and provide features, and system programs that aid in managing \\nthe system while it is running.  \\nComputers use the binary system, which is a number system based on two \\nsymbols: 0 and 1. All information processed by a computer, including data, \\ninstructions, and even the characters we see on the screen, is ultimately \\nrepresented in binary form. This is the reason why digital electronic circuits, such \\nReeshabh Choudhary \\n \\nas those in a CPU, operate using binary signals (on and off). The CPU (central \\nprocessing unit) is the brain behind the operations.  CPUs are made up of several \\nparts, including the control unit (CU), the arithmetic logic unit (ALU), and the \\nregisters, which work together to execute instructions and perform tasks.  \\nAt the foundational level, CPUs are built from basic components like transistors \\nthat act as switches, toggling between on and off states based on the presence \\nor absence of an input signal. These switches are abstracted into logic gates, such \\nas AND, OR, and NOT gates, which form the building blocks for basic Boolean \\nlogic operations within a CPU. These logic gates are constructed by arranging \\ntransistors in specific configurations to implement the desired logical function. \\nFor example, an AND gate outputs a high signal (1) only when all of its input \\nsignals are high (1). By combining different arrangements of transistors, various \\ntypes of logic gates can be created to perform different logical operations. \\n \\nFigure 2-2 CPU Architecture \\nLogic gates serve as the building blocks for more complex logic circuits. These \\ncircuits can implement functions beyond basic Boolean operations, such as \\narithmetic operations, memory storage, and data processing. Components like \\nadders, multiplexers, flip-flops, and registers are constructed using combinations \\nof logic gates. By combining these and other functional blocks, CPU designers \\ncan create custom execution units tailored to specific computational tasks. One \\nof the most critical execution units in a CPU is the Arithmetic Logic Unit (ALU), \\nwhich performs arithmetic and logical operations on binary data. \\nConcurrency & Parallelism \\n \\n28 \\n \\nThe Arithmetic Logical Unit is a digital electronic circuit that performs arithmetic \\nand logical operations on integer binary numbers. Following is the symbolic \\nrepresentation of an ALU: \\n \\nFigure 2-3 ALU \\nIn the given diagram, A, B and Y are three parallel data buses. Each data bus is a \\ngroup of signals that conveys one binary integer number. A and B takes input and \\ndepending on opcode i.e., addition, multiplication etc. Y gets the output. Opcode \\nis, a machine level language, represented by particular combinations of bits. \\nOpcodes are hardwired onto the CPU known as instruction set. \\nSoftware developers write code in high-level programming languages such as C, \\nPython, or Java, which is then compiled into machine code by a compiler. The \\ncompiler translates the high-level code into a sequence of instructions \\nconforming to the Instruction Set Architecture of the target CPU such as x86 \\n(based on CISC). The Instruction Set Architecture (ISA) serves as an interface \\nbetween the software and hardware layers of a computing system, like a \\nlanguage that both the software and hardware components of a computer can \\nunderstand. It specifies the set of instructions that a CPU can execute and how \\nthose instructions are encoded in binary format. This provides a standard way \\nfor software developers to write programs without needing to understand the \\nintricacies of the underlying hardware and paves the path for software \\nportability across different hardware implementations as long as they support \\nthe same ISA. \\nReeshabh Choudhary \\n \\nWhen the CPU fetches an instruction from memory, it reads the instruction set \\nand uses it to determine which operation to perform. This process is known as \\nopcode decoding. Once the opcode is decoded, the CPU executes the \\ncorresponding operation, either directly or by invoking the necessary \\nmicrooperations within the CPU’s control and execution units. The ALU forms the \\ncore of the CPU’s computational capabilities, executing instructions and \\nperforming calculations as directed by the CPU's control unit. \\nThe Control unit directs the operation of other units within the CPU by providing \\ntiming and control signals. It manages the flow of data between the CPU and \\nother devices, such as memory, input/output devices, and the ALU. It directs the \\ncomputer’s memory, arithmetic logic unit and input and output devices how to \\nrespond to the instructions that have been sent to the processor. \\nRegisters are the fastest form of memory in a computer system. They can be \\naccessed in a single clock cycle, which is significantly faster than accessing data \\nfrom main memory. They are located directly within the CPU, which makes them \\nimmediately accessible to the processor without having to access external \\nmemory devices. However, registers have a limited capacity compared to other \\nforms of memory. Modern CPUs typically have a small number of general-\\npurpose registers available for storing data and instructions. Also, they are quite \\ncostly than other forms of memory. \\n2.1 Evolution of Computer Architecture \\nPrimitive computers were very large in size, slow and fragile. They used vacuum \\ntube technology, which was state-of-the-art at the time but made the machine \\nlarge, power-hungry, and prone to frequent failures due to the unreliability of \\nvacuum tubes. Programming these huge devices was a painstaking process. \\nOperators had to physically rewire the machine and set switches for each new \\ncalculation or task, which was a time-consuming and error-prone process.  \\nForget about reprogramming! \\nIn 1940s, Von Neumann’s proposed architectural changes laid the foundation for \\nmodern computing architectures. He introduced the concept of stored program \\ncomputers, that can be programmed to perform multiple tasks and have a \\nmemory unit attached to it. Most of the modern CPU architecture what we see \\ntoday is based on the design proposed by him. With advancement in \\nsemiconductor technology, transistors replaced the bulky and less reliable \\nConcurrency & Parallelism \\n \\n30 \\n \\nvacuum tubes used in early computer designs, which resulted in creation of \\nsmaller, faster, and more reliable computers. \\nModern CPU architectures are implemented on integrated circuits, commonly \\nknown as microprocessor chips. These chips typically consist of one or two \\nmetal-oxide-semiconductor chips, which contain the various components of the \\nCPU, including the control unit, arithmetic logic unit (ALU), registers, and cache \\nmemory, among other components. In recent years, there has been a trend \\ntowards designing microprocessor chips with multiple CPU cores on a single chip. \\nThese chips are referred to as multi-core processors. Each CPU core within a \\nmulti-core processor operates independently and can execute its own set of \\ninstructions. By integrating multiple CPU cores onto a single chip, multi-core \\nprocessors can significantly increase computational performance and efficiency, \\nparticularly for parallelizable tasks such as multitasking, multimedia processing, \\nand scientific simulations. \\nIn embedded systems, commonly used in in consumer electronics, automotive \\nsystems, industrial automation, a microprocessor chip contains a CPU core, \\nmemory (both RAM and ROM or Flash), input/output ports, timers, and other \\nperipheral interfaces, and is known as microcontroller.  They are designed to be \\nlow-cost, low-power, and highly integrated solutions for controlling and \\nmanaging various tasks within an embedded system. We also have System on a \\nChip (SoC) integrates not only the CPU core but also additional components such \\nas graphics processing units (GPUs), memory controllers, peripheral interfaces \\n(e.g., USB, Ethernet, HDMI), and sometimes even wireless communication \\nmodules (e.g., Wi-Fi, Bluetooth) onto a single chip. SoCs are commonly used in \\nsmartphones, tablets, smart TVs, gaming consoles, and other computing devices \\nwhere space, power efficiency, and integration are crucial. \\nAs computer systems have evolved over time, their internal architecture has \\nchanged dramatically over time. And the changes have been aimed at improving \\nperformance of the system. In the 1980s, microprocessors typically followed a \\nmore sequential execution model compared to modern processors. The process \\nof fetching, decoding, and executing an instruction was typically done one at a \\ntime and depending on the complexity of the instruction and the architecture of \\nthe processor, it could take several clock cycles to complete each stage before \\nmoving on to the next instruction. \\nModern systems evolved to execute multiple instructions at a time using \\ntechniques such as pipelining, superscalar execution, out of order execution, \\nReeshabh Choudhary \\n \\nspeculative execution and hyperthreading. These advancements have enabled \\nmodern CPUs to execute instructions much more quickly and efficiently than \\ntheir predecessors. As a result, the number of instructions executed per unit \\ntime, or instructions per cycle (IPC), has increased dramatically. However, in \\ncontrast to CPU performance, improvements in memory latency have been \\nrelatively modest. While memory capacities have grown substantially in \\naccordance with Moore’s Law, the speed at which data can be accessed from \\nmemory has not increased at the same rate. This is primarily due to physical \\nlimitations in memory technology (heard of speed of light delay?), such as the \\nspeed of DRAM (Dynamic Random Access Memory) cells and the limitations of \\nmemory bus bandwidth. \\nSpeed of a computer is about how fast it can move information from one place \\nto another, which basically means how fast a computer can move electrons \\nwithin itself. So, the physical limit of an electron moving through the matter is \\ndefinitely a determinant in the speed limits of a computer system. And speed of \\nelectrons cannot surpass speed of the light. What this means that the when the \\nCPU needs to access data from memory, memory cells closer to the CPU will \\ninherently have lower latency compared to those farther away. \\nIn computer systems, memory modules are typically located at varying distances \\nfrom the CPU. Memory cells closer to the CPU are physically nearer, while those \\nfarther away are physically more distant. \\nModern processors utilize on-chip cache memories to alleviate memory \\nbottlenecks. Caches help reduce the number of variables that need to be \\naccessed from the main memory (DRAM) by storing frequently accessed data \\nand instructions closer to the processor cores, which reduces the need to access \\ndata from slower off-chip memory. This improves the overall system \\nperformance by exploiting the principle of locality, where programs tend to \\naccess the same set of memory locations repeatedly within a short period of \\ntime.  \\nTo balance memory size and access speed, modern processors employ a \\nhierarchical structure of caches. This hierarchy consists of multiple cache levels, \\neach with varying sizes, latencies, and access speeds. The numbering convention \\nfor cache levels reflects their proximity to the processor. Caches closer to the \\nprocessor core have lower latency and higher bandwidth but are smaller in size \\ncompared to caches farther away. \\nConcurrency & Parallelism \\n \\n32 \\n \\nFor instance, L1 Cache is the cache directly attached to a processor core. It \\noperates at a speed close to that of the processor, offering low latency and high \\nbandwidth. However, due to its proximity, it is small in size, typically ranging from \\n16 to 64 KB. L1 caches are dedicated to each processor core and store frequently \\naccessed data and instructions. L2 caches are larger than L1 caches and are often \\nshared among multiple processor cores or streaming multiprocessors in GPU \\narchitectures. They typically range in size from a few hundred kilobytes to a few \\nmegabytes. Although larger in size, L2 caches have higher latency compared to \\nL1 caches. Some high-end processors feature an additional level of cache known \\nas L3 cache. L3 caches are even larger than L2 caches, potentially ranging from \\nhundreds of megabytes to a few gigabytes in size. They provide further caching \\nfor data shared among multiple processor cores or streaming multiprocessors, \\nwhich helps in reducing memory access latency and improving overall system \\nperformance. \\n \\nFigure 2-4 Cache Level Hierarchy \\nCaches exploit the principle of temporal and spatial locality, ensuring that data \\naccessed recently, or data located near recently accessed data is readily available \\nReeshabh Choudhary \\n \\nto the CPU. This reduces the frequency of memory accesses and helps to bridge \\nthe performance gap between CPU and memory.  \\nHowever, even with cache memory, there can still be delays when accessing data \\nfrom main memory (RAM) or other components of the system, especially if the \\ndata needs to travel over relatively long distances on the motherboard or \\nbetween different components. This delay is called “Speed of Light delays”. \\nLet us inspect this further. \\nThe speed of light in a vacuum is indeed approximately 299,792,458 meters per \\nsecond. It is incredibly fast speed; however, it is still limited. In computer \\nsystems, we measure speed in terms of clock frequency, that is how many clock \\ncycles occur per second. For example, a system running at 1.8 GHz has a clock \\nfrequency of 1.8 billion cycles per second. Light can only manage about an 8-\\ncentimeter round trip during a 1.8 GHz clock period. The fastest CPUs these days \\nhave a clock speed of about 5 GHz. This round-trip distance drops further to \\nabout 3 centimetres for a 5 GHz clock. How do we explain this? \\nTo calculate the round-trip distance that light can travel during the duration of a \\nclock period, we need to consider both the propagation speed of light and the \\nduration of a clock cycle. Since light travels at a finite speed, it can only cover a \\ncertain distance within a given time frame. As the clock frequency increases, the \\nduration of each clock cycle decreases. Consequently, light has less time to travel \\nduring each clock cycle, resulting in a shorter round-trip distance. This is why the \\nround-trip distance decreases as the clock frequency increases. \\nNow to make matter worse, electrons don’t flow in vacuum inside computer \\nsystems but through silicon. While light travels quickly in a vacuum, electric \\nwaves in silicon move much more slowly, typically three to thirty times slower \\nthan light in a vacuum. This slower movement of electric waves within silicon \\nfurther constrains the speed at which signals can propagate within a computer \\nsystem. For example, a memory reference may need to wait for a local cache \\nlookup to complete before the request may be passed on to the rest of the \\nsystem. Additionally, relatively low-speed and high-power drivers are required \\nto move electrical signals from one silicon die to another, such as to \\ncommunicate between a CPU and main memory. \\nWhat is implied here that CPU clock frequencies can’t go higher infinitely. Now \\nyou can relate why choosing the right location for your data center is important. \\nConcurrency & Parallelism \\n \\n34 \\n \\nEven with faster processors and improved memory architectures, the memory \\nhierarchy — ranging from registers to caches to RAM to storage — will continue \\nto exist. Caches serve as a critical component of this hierarchy by providing faster \\naccess to frequently accessed data. While CPUs are getting faster, memory \\naccess times are not improving at the same rate. Caches help mitigate the \\nlatency gap between the CPU and main memory by storing frequently accessed \\ndata closer to the CPU, reducing the need to access slower main memory \\nfrequently. \\nHowever, when it comes to common operations such as traversing a linked list, \\ncomputer systems have to deal with unpredictable memory access patterns. \\nLinked lists, especially those with nodes scattered across memory, exhibit poor \\nspatial locality since accessing one node does not imply proximity to other \\nnodes. Hence, traversing the list may cause cache thrashing, where cache lines \\nare continuously replaced as new nodes are accessed. Each cache miss incurred \\nduring traversal may result in loading new cache lines into the cache, only to \\nevict them shortly afterward due to subsequent accesses to different memory \\nlocations. This constant churn of cache lines leads to poor cache utilization and \\ncan negate the benefits of caching. Additionally, linked list traversal may not \\nexploit parallelism effectively, as each node’s traversal typically depends on the \\ncompletion of the previous node’s traversal. This sequential dependence limits \\nthe ability of modern CPUs to execute instructions in parallel and fully utilize \\ntheir multiple execution units. \\n2.2 Interaction with a CPU Core \\nA CPU core is the physical processing unit of a central processing unit (CPU). It is \\nthe component responsible for executing instructions and performing \\ncalculations. Back in the day, computer CPUs used to have single CPU core. In \\nmodern computer systems, a CPU can have multiple cores and several device \\ncontrollers connected through a common bus that provides access between \\ncomponents and shared memory. The more cores a CPU has, the more \\nindependent tasks the processor can run simultaneously. \\nTo effectively understand how OS works in conjunction with CPU, let us observe \\nhow the users interact with a system from the perspective of a single core CPU. \\nTypically, there is a device driver for each device controller, which provides a \\nuniform interface (abstraction) to the rest of the operating system for the \\ncorresponding external device. The OS relies on device drivers to interact with \\nReeshabh Choudhary \\n \\nperipheral devices. Each device controller manages a specific type of device e.g., \\na disk drive, audio device, or graphics display.  \\nAny I/O operation (transfer of data to or from a computer system) is initiated by \\nmaking a request to a specific device driver, which loads the appropriate \\nregisters in the device controller with the necessary information such type of \\noperation to be performed, memory addresses, etc. The device controller, a \\nhardware component responsible for managing the specific I/O device, \\nexamines the contents of the registers to determine the action it needs to \\nperform (e.g., reading a character from the keyboard). It starts the transfer of \\ndata between the I/O device and its local buffer. This is often a time-consuming \\nprocess, especially while dealing with slower peripheral devices like disk drives. \\nOnce the transfer of data is complete or a certain condition is met, the device \\ncontroller signals the CPU through via the system bus, which is the primary \\ncommunication path between major components in a computer system, to \\nnotify about events that require immediate attention. These signals are termed \\nas interrupts. Interrupts can also be generated internally due to exceptions in \\ncomputation or trying to access restricted memory. \\n \\nFigure 2-5 Computer Organization \\nWhen CPU receives an interruption, it stops its current task, saves its state (more \\non this in upcoming section) and transfers control to fixed location in memory \\nknown as the Interrupt Vector Table (IVT). The IVT contains a list of addresses \\ncorresponding to different types of interrupts. Each entry in the IVT points to the \\nConcurrency & Parallelism \\n \\n36 \\n \\nstarting address of the ISR associated with a specific interrupt type. The CPU \\nretrieves the address of the Interrupt Service Routine (ISR) associated with the \\nreceived interrupt from the IVT and jumps to this address to start executing the \\ncode within the ISR.  \\nISR is part of the device drivers responsible for managing communication \\nbetween hardware devices and the operating system. and performs actions \\nrelated to the interrupt, such as processing data, handling I/O operations, or \\nresponding to specific events. When an interrupt occurs, indicating an event that \\nrequires immediate attention, the CPU transfers control to the ISR associated \\nwith the device that triggered the interrupt. The ISR then performs tasks specific \\nto the device, such as reading data from or writing data to the device, \\nacknowledging the interrupt, or initiating further processing. It executes the \\nassigned tasks in a timely manner to ensure that the interrupt is handled \\nefficiently, and the system’s responsiveness is not compromised.  \\n \\nFigure 2-6 Interrupt Handling mechanism \\nOnce the ISR completes its execution, it returns control back to the CPU, which \\nthen restores its saved state, including the program counter and register values \\nand continues execution of the interrupted task from where it left off. \\nThe above process where CPU saves its context to switch over to \\nperform a different task is called context switching. This form of \\ncommunication between them works well for transfer of small \\namounts of data, however for modern systems, there is a huge \\noverhead, as the system is doing no useful work while context \\nswitching. The time taken in context switch varies from machine \\nReeshabh Choudhary \\n \\nto machine and is dependent on memory speed, number of \\nregisters to be copies, etc.  \\nEspecially while transferring bulk data between computer’s main memory and \\nNon-volatile storage devices, there will be interrupts generated for each byte of \\ndata transferred. As a result, each interrupt triggers a context switch, where the \\nCPU switches between the normal execution of the program and the ISR \\nassociated with the I/O operation. Also, CPU is actively involved in managing and \\noverseeing each byte transfer. This involvement includes updating pointers, \\nhandling interrupts, and ensuring data integrity. To overcome this overhead, \\nDirect Memory Access (DMA) is commonly employed, as it allows for the \\nefficient transfer of entire data blocks between the NVS device and main \\nmemory with minimal CPU intervention, reducing the CPU overhead and \\nimproving I/O transfer rates. \\nBefore initiating data transfers, the CPU sets up buffers, pointers, and counters \\nfor the I/O device in coordination with the DMA controller. The DMA controller \\nis a specialized hardware component that has its own set of registers to control \\nthe transfer parameters and operates independently of the CPU once \\nconfigured. Once the setup is complete, the device controller, under the control \\nof the DMA controller, transfers an entire block of data directly between the I/O \\ndevice and main memory. Unlike interrupt driven I/O, where each byte transfer \\nmight trigger an interrupt, DMA generates only one interrupt per data block. \\nWhile the DMA controller is handling the data transfer, the CPU is freed from \\ninvolvement in the process and can perform other tasks concurrently, enhancing \\nthe overall system efficiency and throughput. \\nConcurrency & Parallelism \\n \\n38 \\n \\n \\nFigure 2-7 Dynamic Memory Allocation for bulk data transfer \\nIt must be clear from our discussion that a computer with single CPU core can \\nperform tasks concurrently by context switching or via DMA. Concurrency means \\nall processes(tasks) progressing together but at one time only one task being \\nexecuted by the CPU core. Tasks compete for CPU time. Traditional systems \\nevolved from using one CPU core to employing multiple CPU cores with each \\nCPU core performing tasks in parallel, and thus allowing for more scope for multi-\\ntasking. Each CPU core can be involved in execution of separate process in \\nparallel. Notice the gradual evolution from concurrency to parallelism.  In a \\nmulti-core system, multiple processor cores operate parallel, which reduces \\ncontention for the CPU among different tasks and helps distribute the workload. \\nModern systems take a mixed approach of using concurrency on one core and \\ndistributing tasks across multiple cores (parallelism). \\nSo far in our discussion, we have talked about how a computer or OS reacts to \\nthe events generated by users and processes the instructions. But what does it \\nmean by execution of an instruction or performing a task? Let us shift our \\ndiscussion briefly towards what a process is and then we shall resume our \\ndiscussion related to multiple processes executing together. \\n2.3 Process \\nIn the computing world, we provide the computer with a set of instructions as a \\nprogram, to execute a specific task, which is stored in the secondary memory \\nlike a hard disk as a passive entity. When we give command to the computer to \\nReeshabh Choudhary \\n \\nexecute a specific task, CPU loads the related instructions in the main memory \\nand becomes an active entity, called as process.  \\nAn example of a program is a word processing application, a web browser, or a \\nvideo game. These programs are sets of instructions that define how a computer \\nshould behave when running the corresponding applications. \\nProcess is an instance of a program running on a computer. It is a dynamic entity \\nthat occupies system resources such as memory, CPU time, and I/O channels. A \\nprocess includes the program code and data and additional information required \\nfor its management by the operating system.  \\nEach process has an organized address space and is utilized during execution. \\nThe memory layout of a process is divided into multiple sections as shown \\nbelow: \\n\\uf0b7 The text section (code segment) of the memory layout contains the \\nexecutable code of the program. During execution, the CPU fetches \\ninstructions from this section, advancing the program counter to execute \\neach instruction. The Program Counter (PC) is a special-purpose register \\n(a small, fast-access storage location that holds data temporarily during \\nprocessing) in a computer’s central processing unit (CPU) that keeps track \\nof the address of the next instruction to be executed in a program. It is \\ncrucial for controlling the flow of the program in a sequential manner. \\nDuring a context switch between different processes in a multitasking \\nenvironment, the contents of the Program Counter are saved and later \\nrestored to ensure that each process resumes execution from the correct \\npoint. \\n\\uf0b7 The data section holds initialized global and static variables. These \\nvariables retain their values throughout the program’s execution. This \\nsection is crucial for storing data that persists across function calls. \\n\\uf0b7 The stack is used to manage function calls and store local variables. Each \\nfunction call results in a new stack frame (activation record) being pushed \\nonto the stack. Local variables and function parameters are stored within \\nthese stack frames. \\n\\uf0b7 The heap is utilized for dynamic memory allocation, allowing the program \\nto request memory at runtime using functions like malloc or new (in \\nlanguages like C or C++). The heap accommodates data structures that can \\ngrow or shrink dynamically during execution. \\nConcurrency & Parallelism \\n \\n40 \\n \\n \\nFigure 2-8 Memory layout of a process \\nThe stack and heap sections in memory are managed by the OS to prevent them \\nfrom overlapping. While it is possible for the stack and heap to overlap, the OS \\ntypically starts them far apart to avoid this. If the stack and heap were to overlap, \\nit could lead to issues such as buffer overflow, unpredictable behavior, and \\nmemory corruption. Modern operating systems use various mechanisms, such \\nas virtual memory guard pages, to prevent the stack from growing into the heap \\nand vice versa. Guard pages are memory pages strategically placed between the \\nstack and heap segments by the operating system during memory allocation. \\nThese pages are marked as inaccessible, which basically means that any attempt \\nto access these guard pages (read/write) will trigger an error, typically a \\nsegmentation fault or a similar memory access violation. So, an effective \\nmemory barrier is created between the stack and heap. Any attempt by a \\nprogram to access memory beyond the stack or heap boundaries, such as by \\nattempting to grow the stack into the heap or vice versa, will trigger an error \\nwhen it encounters the guard page. \\nThe Information about the memory layout of a process is stored in a data \\nstructure called the Process Control Block (PCB). It contains details about the \\nprocess, including the program counter, register values, memory allocation, and \\nstatus. \\nReeshabh Choudhary \\n \\n \\nFigure 2-9 Process Control Block (PCB) \\nLet us look at some important sections of a PCB: \\n\\uf0b7 Process ID: Each process is assigned a unique identifier which helps the \\nOS distinguish it from different processes. \\n\\uf0b7 Process state: Each process goes through various state changes like new, \\nready, running, waiting, etc. and the information is stored in this section. \\n\\uf0b7 Program Counter: We discussed about Program counter above. It is a \\nspecial purpose register which is used to keep track of the address of the \\nnext instruction to be executed. \\n\\uf0b7 CPU registers: Depending upon a computer, there can be various CPU \\nregisters like general-purpose registers, status registers, and other special-\\npurpose registers. Some common registers present across the platforms \\nare accumulators, index registers, stack pointers, etc.  \\n\\uf0b7 CPU Scheduling Information: The information about a process's \\nscheduling state, priority, and other attributes used by the CPU scheduler \\nto determine the order of execution is stored in this section. \\n\\uf0b7 Memory Management Information: It includes critical details about how \\na process interacts with the memory system. The value of base and limit \\nregisters, page tables, etc. are saved in this section. The base register holds \\nthe starting address of the memory block allocated to the process. The \\nlimit register contains the size of the memory block, restricting the process \\nfrom accessing memory beyond this limit. \\nConcurrency & Parallelism \\n \\n42 \\n \\nDuring a context switch, the PCB is used to save the current \\nstate of the process, allowing the operating system to switch to \\nand execute another process. When a process completes its \\nexecution, the PCB is typically updated to reflect its terminated \\nstate. The process’s resources are then released. Information in \\nthe PCB is utilized for inter-process communication (IPC, more \\non this in coming section) and synchronization mechanisms, \\nfacilitating communication between processes. The PCB can be \\nviewed as a snapshot of the process’s state and serve as a \\nreference point for the OS to make decisions regarding \\nscheduling, resource allocation, and process execution. \\nIt is important to note that each process can create several new processes (child \\nprocesses) during its course of execution. When a child process is created, the \\nallocation of resources such as CPU time, memory, file access, etc. is done by the \\nOS dynamically as it adjusts resource limits based on current system load and \\npriorities of processes. These resources might be distributed directly to these \\nprocesses or as a subset of the available resources to the parent process. The \\nlater approach is recommended, as it helps prevent system overload. It allows \\nthe OS to maintain control over resource usage and prevent any single process, \\nwhether poorly designed or malicious, from monopolizing system resources. \\nWithout such restrictions, a poorly designed or malicious process could create \\nan excessive number of child processes, potentially overwhelming the system. \\nUpon process termination, whether voluntarily or due to an error, all resources \\nassociated with the process are deallocated and reclaimed by the OS. If the \\nterminated process has a parent, the operating system may notify the parent \\nabout the completion of the child process, so that the parent can perform any \\nnecessary cleanup or handle the termination event. \\nReeshabh Choudhary \\n \\n \\nFigure 2-10 Context switch between two processes \\nLet us understand this by considering the multi-process architecture of Chromium \\nbrowsers: \\nWhen we launch the browser, it starts a main process (parent process) that \\nmanages the user interface and browser functions. Additionally, Chrome uses a \\nmulti-process architecture where each tab, extension, and plugins run in their \\nown separate processes (child processes). Each open tab in the Chrome browser \\nis associated with a separate renderer process, which manages the rendering \\nand display of web pages, ensuring that each tab runs independently. If one tab \\nmeets an issue (e.g., a crash), it doesn't affect the entire browser. Chrome may \\nalso spawn a separate GPU process to handle graphics-related tasks. There are \\nother processes also running within Chrome browser such as Extension process \\nand utility processes. The list of processes running under Chrome can be viewed \\nby opening its task manager. This design enhances security and stability, as issues \\nwith one tab or process are less likely to affect others. \\nConcurrency & Parallelism \\n \\n44 \\n \\n \\nFigure 2-11 Chromium Multi-process example \\n2.4 Inter-process Communication (IPC) \\nA process which does not share data with any other process is an independent \\nprocess and if a process shares data with other processes in system, it is a \\ncooperating process. Processes not just run parallel but also communicate with \\neach other in the form of exchanging data.  \\nSupplying a suitable environment for inter-process communication is important \\nfor assorted reasons. Sometimes, multiple applications get interested in the \\nsame piece of information or parallelly executing child processes need to be \\ncommunicating with each other to speed up the computation. In general, there \\nare two fundamental models for inter-process communication: shared memory \\nand message passing. \\nShared memory, as the name suggests, allows multiple processes to share a \\ncommon region of memory. In shared memory model, processes must agree to \\nset up a shared-memory region, and they need to coordinate their access to this \\nshared region. One of the processes (typically referred to as the creator or \\nproducer) starts the creation of a shared-memory segment using system calls \\nand this segment exists in the address space of the creating process. Other \\nprocesses (consumers or clients) that wish to communicate using the shared-\\nmemory segment must attach it to their own address space. Once attached, \\nprocesses can read from and write to the shared-memory segment as if it were \\ntheir own memory. Processes using shared memory must coordinate their \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_text = pdf_to_text(\"data/BookPreview.pdf\")\n",
    "result_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing result text from PDF to LLM transformer for deriving relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Reeshabh Choudhary', type='Person'), Node(id='Concurrency & Parallelism', type='Book'), Node(id='Objects, Data & Ai', type='Book'), Node(id='Maa Saraswati', type='Deity'), Node(id='Ganesh Ji', type='Deity'), Node(id='Hanuman Ji', type='Deity'), Node(id='Richa', type='Person'), Node(id='Maithili', type='Person'), Node(id='Ashish Agarwal', type='Person'), Node(id='Gurpreet Singh', type='Person'), Node(id='Saurabh Singh', type='Person'), Node(id='Rupam Das', type='Person'), Node(id='Robert Greene', type='Person'), Node(id='Charles Dickens', type='Person'), Node(id='Fyodor Dostoevsky', type='Person'), Node(id='Oliver Twist', type='Book'), Node(id='Mastery', type='Book')]\n",
      "Relationships:[Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Concurrency & Parallelism', type='Book'), type='AUTHOR'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Objects, Data & Ai', type='Book'), type='AUTHOR'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Maa Saraswati', type='Deity'), type='ACKNOWLEDGES'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Ganesh Ji', type='Deity'), type='ACKNOWLEDGES'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Hanuman Ji', type='Deity'), type='ACKNOWLEDGES'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Richa', type='Person'), type='SPOUSE'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Maithili', type='Person'), type='PARENT'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Ashish Agarwal', type='Person'), type='FRIEND'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Gurpreet Singh', type='Person'), type='FRIEND'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Saurabh Singh', type='Person'), type='FRIEND'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Rupam Das', type='Person'), type='MENTOR'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Robert Greene', type='Person'), type='INSPIRED_BY'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Charles Dickens', type='Person'), type='QUOTES'), Relationship(source=Node(id='Reeshabh Choudhary', type='Person'), target=Node(id='Fyodor Dostoevsky', type='Person'), type='QUOTES'), Relationship(source=Node(id='Charles Dickens', type='Person'), target=Node(id='Oliver Twist', type='Book'), type='AUTHOR'), Relationship(source=Node(id='Robert Greene', type='Person'), target=Node(id='Mastery', type='Book'), type='AUTHOR')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=result_text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the entity count in the Nodes section. LLM generated graph is able to create a very limited number of entites while GRAPHRAG based model is able to create more than 700 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
